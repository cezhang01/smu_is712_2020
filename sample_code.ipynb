{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rmxggiIR1h7Z",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.601888395868E12,
     "user_tz": -480.0,
     "elapsed": 2632.0,
     "user": {
      "displayName": "牛煜东",
      "photoUrl": "",
      "userId": "07288939057450066909"
     }
    },
    "outputId": "8ad069fb-9107-497a-ca33-d47d48763744",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6VG2M8n8iXL"
   },
   "outputs": [],
   "source": [
    "!unzip -qq /content/drive/My\\ Drive/IS712/train.zip -d ./data/\n",
    "!unzip -qq /content/drive/My\\ Drive/IS712/validation.zip -d ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cFFvMcUJzR2w",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.601888663411E12,
     "user_tz": -480.0,
     "elapsed": 14135.0,
     "user": {
      "displayName": "牛煜东",
      "photoUrl": "",
      "userId": "07288939057450066909"
     }
    },
    "outputId": "acac6d41-eae8-41f5-9672-1e6c359ba52a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.18.5\n",
      "TensorFlow version: 2.3.0\n",
      "Training data size: 2400\n",
      "Test data size: 600\n",
      "Inference data size: 1500\n",
      "\n",
      "2020-10-05 09:04:09.259758 Epoch: 1/1\n",
      "Training loss: 40.837032\n",
      "Test loss: 9.391784\n",
      "Test mean RMSE: 2.434575\n",
      "Test stddev RMSE: 0.644863\n",
      "Inference results saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "from os.path import basename\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "random.seed(712)\n",
    "np.random.seed(712)\n",
    "\n",
    "\n",
    "# *****************************************Data Preparation*****************************************\n",
    "TRAIN_DATA_DIR = \"./data/train/\"\n",
    "INFERENCE_DATA_DIR = \"./data/validation/\"\n",
    "TEST_SIZE = 0.2\n",
    "IMG_SIZE = 32\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "LAMBDA_BALANCE = 1.0\n",
    "LAMBDA_REG = 0.0\n",
    "NUM_THREADS = 4\n",
    "\n",
    "\n",
    "def gen_data():\n",
    "\n",
    "    img_paths = [p for p in glob.glob(TRAIN_DATA_DIR + '/*.jpg')]\n",
    "    n_train = int(len(img_paths) * (1 - TEST_SIZE))\n",
    "    train_img_paths = img_paths[:n_train]\n",
    "    test_img_paths = img_paths[n_train:]\n",
    "    inference_img_paths = [p for p in glob.glob(INFERENCE_DATA_DIR + '/*.jpg')]\n",
    "\n",
    "    mean, stddev = [], []\n",
    "    with open(TRAIN_DATA_DIR + '/label_train.txt', 'r') as file:\n",
    "        for row in file:\n",
    "            mean.append(float(row.split('\\t')[0]))\n",
    "            stddev.append(float(row.split('\\t')[1]))\n",
    "    train_mean, train_stddev = mean[:n_train], stddev[:n_train]\n",
    "    test_mean, test_stddev = mean[n_train:], stddev[n_train:]\n",
    "\n",
    "    inference_mean, inference_stddev = np.zeros(len(inference_img_paths)), np.zeros(len(inference_img_paths))\n",
    "\n",
    "    train_data = integrate(train_img_paths, train_mean, train_stddev)\n",
    "    test_data = integrate(test_img_paths, test_mean, test_stddev)\n",
    "    inference_data = integrate(inference_img_paths, inference_mean, inference_stddev)\n",
    "    print('Training data size: {}'.format(len(train_data)))\n",
    "    print('Test data size: {}'.format(len(test_data)))\n",
    "    print('Inference data size: {}'.format(len(inference_data)))\n",
    "\n",
    "    return train_data, test_data, inference_data\n",
    "\n",
    "\n",
    "def integrate(img_paths, mean, stddev):\n",
    "\n",
    "    data = []\n",
    "    for idx in range(len(img_paths)):\n",
    "        data.append([img_paths[idx], mean[idx], stddev[idx]])\n",
    "\n",
    "    return np.asarray(data)\n",
    "\n",
    "\n",
    "### IMAGE READING PARSING ###\n",
    "\n",
    "def read_img(img_path, is_training=False):\n",
    "    img_string = tf.read_file(img_path)\n",
    "    img_decoded = tf.image.decode_jpeg(img_string, channels=3)\n",
    "    img = tf.image.resize_images(img_decoded, [IMG_SIZE, IMG_SIZE])\n",
    "    img = img / 255.0\n",
    "\n",
    "    if is_training:\n",
    "        \"\"\"Data augmentation comes here\"\"\"\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def parse_function(img_path, mean, stddev, is_training=False):\n",
    "\n",
    "    img = read_img(img_path, is_training)\n",
    "\n",
    "    return img, [mean], [stddev]\n",
    "\n",
    "\n",
    "def parse_function_train(img_path, mean, stddev):\n",
    "\n",
    "    return parse_function(img_path, mean, stddev, is_training=True)\n",
    "\n",
    "\n",
    "def parse_function_test(img_path, mean, stddev):\n",
    "\n",
    "    return parse_function(img_path, mean, stddev, is_training=False)\n",
    "\n",
    "\n",
    "### DATA SERVING ###\n",
    "\n",
    "class DataGenerator(object):\n",
    "\n",
    "    def __init__(self, batch_size=1, num_threads=1,\n",
    "                 train_shuffle=False, buffer_size=10000):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_threads = num_threads\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        # data sampling and spliting\n",
    "        self.train_data, self.test_data, self.inference_data = gen_data()\n",
    "\n",
    "        # build iterator\n",
    "        self.train_set = self._build_data_set(self.train_data,\n",
    "                                              parse_function_train,\n",
    "                                              shuffle=train_shuffle)\n",
    "        self.iterator = tf.data.Iterator.from_structure(self.train_set.output_types,\n",
    "                                                        self.train_set.output_shapes)\n",
    "        # for training\n",
    "        self.train_init_op = self.iterator.make_initializer(self.train_set)\n",
    "        self.next = self.iterator.get_next()\n",
    "        self.num_train_batches = int(np.ceil(len(self.train_data) / batch_size))\n",
    "        # for testing\n",
    "        self.test_set = self._build_data_set(self.test_data, parse_function_test)\n",
    "        self.test_init_op = self.iterator.make_initializer(self.test_set)\n",
    "        self.num_test_batches = int(np.ceil(len(self.test_data) / batch_size))\n",
    "        # for inference\n",
    "        self.inference_set = self._build_data_set(self.inference_data, parse_function_test)\n",
    "        self.inference_init_op = self.iterator.make_initializer(self.inference_set)\n",
    "        self.num_inference_batches = int(np.ceil(len(self.inference_data) / batch_size))\n",
    "\n",
    "    def _build_data_set(self, data, map_fn, shuffle=False):\n",
    "        \"\"\"\n",
    "        Images are loaded from disk and processed batch by batch. Since our dataset\n",
    "        is not that big, it would be faster if we load all the images into RAM once\n",
    "        and read from their. I leave it for you guys to explore :)\n",
    "        \"\"\"\n",
    "        img_path = tf.convert_to_tensor(data[:, 0], dtype=tf.string)\n",
    "        mean = tf.convert_to_tensor(data[:, 1], dtype=tf.float64)\n",
    "        stddev = tf.convert_to_tensor(data[:, 2], dtype=tf.float64)\n",
    "        data = tf.data.Dataset.from_tensor_slices((img_path, mean, stddev))\n",
    "        if shuffle:\n",
    "            data = data.shuffle(buffer_size=self.buffer_size)\n",
    "        data = data.map(map_fn, num_parallel_calls=self.num_threads)\n",
    "        data = data.batch(self.batch_size)\n",
    "        data = data.prefetch(self.num_threads)\n",
    "        return data\n",
    "\n",
    "\n",
    "# *****************************************Model Architecture*****************************************\n",
    "class MLP(object):\n",
    "\n",
    "    def __init__(self, training=False):\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, [None, IMG_SIZE, IMG_SIZE, 3])\n",
    "        self.y1 = tf.placeholder(tf.float32, [None, 1])\n",
    "        self.y2 = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        net = self._encoder(self.x)\n",
    "\n",
    "        with tf.variable_scope('regression'):\n",
    "            self.mean = tf.layers.dense(net, 1, name='mean')\n",
    "            self.stddev = tf.layers.dense(net, 1, name='stddev')\n",
    "\n",
    "        if training:\n",
    "            self.loss, self.train_op = self._loss_fn()\n",
    "\n",
    "    def _encoder(self, input, name='encoder'):\n",
    "\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            net = tf.layers.flatten(input)\n",
    "            net = tf.layers.dense(net, units=300, activation=tf.nn.relu)\n",
    "            net = tf.layers.dense(net, units=300, activation=tf.nn.relu)\n",
    "\n",
    "            return net\n",
    "\n",
    "    def _loss_fn(self):\n",
    "\n",
    "        trained_vars = tf.trainable_variables()\n",
    "\n",
    "        error_mean = tf.sqrt(tf.reduce_mean((self.y1 - self.mean) ** 2))\n",
    "        error_stddev = tf.sqrt(tf.reduce_mean((self.y2 - self.stddev) ** 2))\n",
    "\n",
    "        l2_reg = tf.add_n([tf.nn.l2_loss(v) for v in trained_vars\n",
    "                           if 'bias' not in v.name])\n",
    "        loss = error_mean + LAMBDA_BALANCE * error_stddev + LAMBDA_REG * l2_reg\n",
    "\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(LEARNING_RATE,\n",
    "                                           beta1=0.9,\n",
    "                                           beta2=0.99,\n",
    "                                           epsilon=1e-8)\n",
    "        train_op = optimizer.minimize(loss)#, global_step, var_list=trained_vars)\n",
    "\n",
    "        return loss, train_op\n",
    "\n",
    "\n",
    "# *****************************************Training, Test, and Inference*****************************************\n",
    "generator = DataGenerator(batch_size=BATCH_SIZE, num_threads=NUM_THREADS, train_shuffle=True, buffer_size=10000)\n",
    "model = MLP(training=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(\"\\n{} Epoch: {}/{}\".format(datetime.now(), epoch, NUM_EPOCHS))\n",
    "\n",
    "        # **********************Training**********************\n",
    "        sum_loss = 0.\n",
    "        sess.run(generator.train_init_op)\n",
    "        for step in range(generator.num_train_batches):\n",
    "            batch_img, batch_mean, batch_stddev = sess.run(generator.next)\n",
    "            _, loss = sess.run([model.train_op, model.loss], feed_dict={model.x: batch_img,\n",
    "                                                                        model.y1: batch_mean,\n",
    "                                                                        model.y2: batch_stddev})\n",
    "            sum_loss += loss\n",
    "        print('Training loss: {:.6f}'.format(sum_loss))\n",
    "\n",
    "        # **********************Test**********************\n",
    "        pred_means = []\n",
    "        pred_stddevs = []\n",
    "        true_means = []\n",
    "        true_stddevs = []\n",
    "        sum_loss = 0.\n",
    "        sess.run(generator.test_init_op)\n",
    "        for step in range(generator.num_test_batches):\n",
    "            batch_img, batch_mean, batch_stddev = sess.run(generator.next)\n",
    "            pred_mean, pred_stddev, loss = sess.run([model.mean, model.stddev, model.loss], feed_dict={model.x: batch_img,\n",
    "                                                                                                       model.y1: batch_mean,\n",
    "                                                                                                       model.y2: batch_stddev})\n",
    "            sum_loss += loss\n",
    "            pred_means.extend(pred_mean.ravel().tolist())\n",
    "            pred_stddevs.extend(pred_stddev.ravel().tolist())\n",
    "            true_means.extend(batch_mean.ravel().tolist())\n",
    "            true_stddevs.extend(batch_stddev.ravel().tolist())\n",
    "        pred_means = np.asarray(pred_means)\n",
    "        pred_stddevs = np.asarray(pred_stddevs)\n",
    "        true_means = np.asarray(true_means)\n",
    "        true_stddevs = np.asarray(true_stddevs)\n",
    "        print('Test loss: {:.6f}'.format(sum_loss))\n",
    "        print('Test mean RMSE: {:.6f}'.format(np.sqrt(np.mean((pred_means - true_means) ** 2))))\n",
    "        print('Test stddev RMSE: {:.6f}'.format(np.sqrt(np.mean((pred_stddevs - true_stddevs) ** 2))))\n",
    "\n",
    "        # **********************Inference**********************\n",
    "        if not os.path.exists('./submission'):\n",
    "            os.makedirs('./submission')\n",
    "        inference_means = []\n",
    "        inference_stddevs = []\n",
    "        sum_loss = 0.\n",
    "        sess.run(generator.inference_init_op)\n",
    "        for step in range(generator.num_inference_batches):\n",
    "            batch_img, batch_mean, batch_stddev = sess.run(generator.next)\n",
    "            pred_mean, pred_stddev, _ = sess.run([model.mean, model.stddev, model.loss], feed_dict={model.x: batch_img,\n",
    "                                                                                                    model.y1: batch_mean,\n",
    "                                                                                                    model.y2: batch_stddev})\n",
    "            inference_means.extend(pred_mean.ravel().tolist())\n",
    "            inference_stddevs.extend(pred_stddev.ravel().tolist())\n",
    "        inference_means = np.asarray(inference_means)\n",
    "        inference_stddevs = np.asarray(inference_stddevs)\n",
    "\n",
    "        # Output\n",
    "        with open('./submission/prediction.txt', 'w') as file:\n",
    "            for idx in range(len(inference_means)):\n",
    "                file.write(str(inference_means[idx]))\n",
    "                file.write('\\t')\n",
    "                file.write(str(inference_stddevs[idx]))\n",
    "                file.write('\\n')\n",
    "        ZipFile('./submission/prediction.zip', 'w').write('./submission/prediction.txt', basename('prediction.txt'))\n",
    "        print('Inference results saved!')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyMblXBiP9tt2DAFFamvGIDI"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
